# Snakefile

def get_haps_for_sample(sample):
    return list(range(1, config["samples"][sample]["ploidy"] + 1))

rule all:
    input:
        expand(config["workdir"] + "/{sample}/hifiasm_complete.txt", sample=config["samples"].keys()),
        expand(config["workdir"] + "/{sample}/02_priHapHiC/04.build/scaffolds.fa", sample=config["samples"].keys()),
        expand(config["workdir"] + "/{sample}/02_HapHiC/haphic_complete.txt", sample=config["samples"].keys()),

# Make assemblies
rule hifiasm_hic:
    input:
        hifi = lambda wildcards: config["samples"][wildcards.sample]["hifi"],
        hic_r1 = lambda wildcards: config["samples"][wildcards.sample]["hic_r1"],
        hic_r2 = lambda wildcards: config["samples"][wildcards.sample]["hic_r2"]
    output:
        pri_asm = config["workdir"] + "/{sample}/{sample}.hic.p_ctg.fa",
        hap_files = expand(config["workdir"] + "/{sample}/{sample}.hic.hap{hap}.p_ctg.fa", 
            sample="{sample}", hap=[i for i in range(1, config["samples"][list(config["samples"].keys())[0]]["ploidy"] + 1)]),
        #hap_files = expand(config["workdir"] + "/{sample}/{sample}.hic.hap{hap}.p_ctg.fa", 
        #    hap=[i for i in range(1, config["samples"][list(config["samples"].keys())[0]]["ploidy"] + 1)]),
        complete = touch(config["workdir"] + "/{sample}/hifiasm_complete.txt"),
        outlog = config["workdir"] + "/{sample}/{sample}.hifiasm.log"
    params:
        ploidy = lambda wildcards: config["samples"][wildcards.sample]["ploidy"],
        outprefix = config["workdir"] + "/{sample}/{sample}",
        hom_cov = lambda wildcards: config["samples"][wildcards.sample]["hom_cov"]
    threads: 64
    resources:
        mem_mb=500000  # 500GB in MB
    singularity: config["container"]
    shell:
        """
        echo "~~~~ Starting HiFiasm-HiC Assembly for {wildcards.sample} ~~~~"
        hifiasm --n-hap {params.ploidy} -t {threads} \
            --h1 {input.hic_r1} --h2 {input.hic_r2} \
            -o {params.outprefix} --hom-cov {params.hom_cov} {input.hifi} > {output.outlog} 2>&1
        awk '/^S/{{print ">"$2;print $3}}' {params.outprefix}.hic.p_ctg.gfa > {params.outprefix}.hic.p_ctg.fa
        for HAP in $(seq 1 {params.ploidy}); do
            awk '/^S/{{print ">"$2; print $3}}' {params.outprefix}.hic.hap${{HAP}}.p_ctg.gfa > {params.outprefix}.hic.hap${{HAP}}.p_ctg.fa
        done
        echo "~~~~ HiFiasm-HiC Assembly Complete ~~~~"
        """

rule haphic_primary:
    input:
        pri_asm = config["workdir"] + "/{sample}/{sample}.hic.p_ctg.fa",
        hic_r1 = lambda wildcards: config["samples"][wildcards.sample]["hic_r1"],
        hic_r2 = lambda wildcards: config["samples"][wildcards.sample]["hic_r2"]
    output:
        scaffolds = config["workdir"] + "/{sample}/02_priHapHiC/04.build/scaffolds.fa"
    params:
        nchrs = lambda wildcards: config["samples"][wildcards.sample]["nchrs"],
        ploidy = lambda wildcards: config["samples"][wildcards.sample]["ploidy"],
        work_dir = config["workdir"] + "/{sample}/02_priHapHiC"
    threads: 64
    resources:
        mem_mb=500000
    singularity: config["container"]
    shell:
        """
        echo "~~~~ Scaffolding with HapHiC Primary Assembly {wildcards.sample} ~~~~"
        # Create and enter working directory
        mkdir -p {params.work_dir}
        cd {params.work_dir}

        rm -rf 01.* 02.* 03.* 04.*

        # Index merged assembly
        bwa index {input.pri_asm}

        # Align Hi-C reads
        bwa mem -5SP -t {threads} {input.pri_asm} {input.hic_r1} {input.hic_r2} | \
            samblaster | samtools view - -@ {threads} -S -h -b -F 3340 -o HiC.bam

        # Filter alignments
        filter_bam HiC.bam 1 --nm 3 --threads {threads} --remove-dup | \
            samtools view - -b -@ {threads} -o HiC.filtered.bam

        # Run HapHiC pipeline
        haphic pipeline {input.pri_asm} \
            HiC.filtered.bam \
            {params.nchrs} \
            --correct_nrounds 2 \
            --threads {threads} \
            --processes {threads} \
            --max_inflation 10.0 \
            --remove_allelic_links {params.ploidy}

        echo "~~~~ Scaffolding with HapHiC Primary Assembly {wildcards.sample} complete ~~~~"
        """

rule haphic_haps:
    input:
        hic_r1 = lambda wildcards: config["samples"][wildcards.sample]["hic_r1"],
        hic_r2 = lambda wildcards: config["samples"][wildcards.sample]["hic_r2"],
        reference = lambda wildcards: config["samples"][wildcards.sample]["reference"]
    output:
        completed = touch(config["workdir"] + "/{sample}/02_HapHiC/haphic_complete.txt")
    params:
        nchrs = lambda wildcards: config["samples"][wildcards.sample]["nchrs"],
        ploidy = lambda wildcards: config["samples"][wildcards.sample]["ploidy"],
        work_dir = config["workdir"] + "/{sample}/02_HapHiC",
        hap_dir = config["workdir"] + "/{sample}/"
    threads: 64
    resources:
        mem_mb=500000
    singularity: config["container"]
    shell:
        """
        echo "~~~~ Scaffolding with HapHiC {wildcards.sample} ~~~~"
        # Create and enter working directory
        mkdir -p {params.work_dir}
        cd {params.work_dir}
        set +e

        # Process each haplotype
        for HAP in $(seq 1 {params.ploidy}); do

            echo "~~~~ Scaffolding Haplotype ${{HAP}} with HapHiC {wildcards.sample} ~~~~"
            mkdir -p hap${{HAP}}
            cd hap${{HAP}}
            
            # Index merged assembly
            bwa index {params.hap_dir}/{wildcards.sample}.hic.hap${{HAP}}.fa

            # Align Hi-C reads
            bwa mem -5SP -t {threads} {params.hap_dir}/{wildcards.sample}.hic.hap${{HAP}}.fa {input.hic_r1} {input.hic_r2} | \
                samblaster | samtools view - -@ {threads} -S -h -b -F 3340 -o HiC.bam

            # Filter alignments
            filter_bam HiC.bam 1 --nm 3 --threads {threads} --remove-dup | \
                samtools view - -b -@ {threads} -o HiC.filtered.bam

            rm -rf 01* 02* 03* 04*
            
            # Run HapHiC pipeline
            haphic pipeline {params.hap_dir}/{wildcards.sample}.hic.hap${{HAP}}.fa \
                HiC.filtered.bam \
                {params.nchrs} \
                --correct_nrounds 2 \
                --threads {threads} \
                --processes {threads} \
                --max_inflation 10.0 \
                --remove_allelic_links {params.ploidy}
            
            # Check if the pipeline was successful
            if [ $? -eq 0 ]; then
                cp 04.build/scaffolds.fa ../hap${{HAP}}.fa
            else
                echo "HapHiC pipeline failed for haplotype ${{HAP}}"
            fi

        done

        set -e
        echo "~~~~ Scaffolding with HapHiC {wildcards.sample} complete ~~~~"
        """

# rule repurge_hic:
#     input:
#         purgehic = config["workdir"] + "/{sample}/HiCHiFi/03_HapHiC/hap{hap}.purged.haphic.fa",
#         # Make sure hifiasm_hic completed
#         haphic_done = config["workdir"] + "/{sample}/HiCHiFi/03_HapHiC/haphic_complete.txt"
#     output:
#         purgedhic = config["workdir"] + "/{sample}/HiCHiFi/04_PurgeDups/hap{hap}/purged.fa"
#     threads: 64
#     resources:
#         mem_mb=500000  # 500GB in MB
#     singularity: config["container"]
#     shell:
#         """
#         echo "~~~~ Starting Second Round Purge_Dups for {wildcards.sample} haplotype {wildcards.hap} ~~~~"
#         mkdir -p $(dirname {output.purgedhic})
#         cd $(dirname {output.purgedhic})
        
#         split_fa {input.purgehic} > asm.split
#         minimap2 -t {threads} -xasm5 -DP asm.split asm.split | gzip -c > asm.split.self.paf.gz
#         purge_dups -M1000 -E1000 asm.split.self.paf.gz > dups.bed
#         get_seqs dups.bed {input.purgehic}
        
#         echo "~~~~ Second Purge_Dups Complete for haplotype {wildcards.hap} ~~~~"
#         """

# rule ragtag:
#     input:
#         purgedhic = config["workdir"] + "/{sample}/HiCHiFi/04_PurgeDups/hap{hap}/purged.fa",
#         reference = lambda wildcards: config["samples"][wildcards.sample]["reference"]
#     output:
#         final_assembly = config["workdir"] + "/Haplotype_Assemblies/{sample}.hap{hap}.purged.haphic.purged.ragtag.fa"
#     params:
#         work_dir = config["workdir"] + "/{sample}/HiCHiFi/05_RagTag/hap{hap}"
#     threads: 64
#     resources:
#         mem_mb=500000  # 500GB in MB
#     singularity: config["container"]
#     shell:
#         """
#         echo "~~~~ Starting RagTag Scaffolding for {wildcards.sample} haplotype {wildcards.hap} ~~~~"
#         mkdir -p {params.work_dir}
#         cd {params.work_dir}
        
#         ragtag.py scaffold {input.reference} {input.purgedhic} -o hap{wildcards.hap} --aligner minimap2 -t {threads}
#         cp hap{wildcards.hap}/ragtag.scaffold.fasta hap{wildcards.hap}.purged.haphic.purged.ragtag.fa
        
#         mkdir -p $(dirname "{output.final_assembly}")
#         cp hap{wildcards.hap}.purged.haphic.purged.ragtag.fa "{output.final_assembly}"

#         echo "~~~~ Ragtag Scaffolding Complete for {wildcards.sample} and haplotype {wildcards.hap} ~~~~"
#         """